{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/t/ts218/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-10 11:41:12.127736: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "# from bert import tokenization\n",
    "# from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/raw to /home/stud/t/ts218/.cache/huggingface/datasets/csv/raw-7d028b0f05d4348c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 1697.87it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 35.05it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/stud/t/ts218/.cache/huggingface/datasets/csv/raw-7d028b0f05d4348c/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 162.42it/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 78.3kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 522kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 2.32MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.60MB/s]\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "# create training dataset\n",
    "data_files = {\"train\": \"train.csv\", \"validation\": \"validation.csv\", \"test\": \"test.csv\"}\n",
    "dataset = datasets.load_dataset(\"./data/raw\", data_files=data_files)\n",
    "\n",
    "# tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data['description'])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 527M/527M [00:04<00:00, 116MB/s]  \n",
      "2023-05-10 11:41:34.417539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 11:41:36.853559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11447 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/stud/t/ts218/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulting batches in each dataset:\n",
      "train: 340\n",
      "validate: 37\n",
      "test: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=1391)\n",
    "\n",
    "tf_train = model.prepare_tf_dataset(dataset['train'],batch_size=10, shuffle=True, tokenizer=tokenizer)\n",
    "tf_val = model.prepare_tf_dataset(dataset['validation'],batch_size=10, shuffle=True, tokenizer=tokenizer)\n",
    "tf_test = model.prepare_tf_dataset(dataset['test'],batch_size=10, shuffle=True, tokenizer=tokenizer)\n",
    "\n",
    "print('resulting batches in each dataset:')\n",
    "print(f'train: {tf_train.cardinality().numpy()}\\nvalidate: {tf_val.cardinality().numpy()}\\ntest: {tf_test.cardinality().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "340/340 [==============================] - 60s 135ms/step - loss: 6.3981 - accuracy: 0.0994 - val_loss: 5.8545 - val_accuracy: 0.1676\n",
      "Epoch 2/5\n",
      "340/340 [==============================] - 45s 133ms/step - loss: 5.5304 - accuracy: 0.1812 - val_loss: 5.5226 - val_accuracy: 0.2297\n",
      "Epoch 3/5\n",
      "340/340 [==============================] - 46s 135ms/step - loss: 5.0858 - accuracy: 0.2312 - val_loss: 5.3924 - val_accuracy: 0.2649\n",
      "Epoch 4/5\n",
      "340/340 [==============================] - 47s 137ms/step - loss: 4.6522 - accuracy: 0.2976 - val_loss: 5.3970 - val_accuracy: 0.2676\n",
      "Epoch 5/5\n",
      "340/340 [==============================] - 47s 138ms/step - loss: 4.2855 - accuracy: 0.3335 - val_loss: 5.2508 - val_accuracy: 0.2892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcdec307040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure callbacks\n",
    "# checkpoint\n",
    "filepath=\"./output/raw/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max', save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# train model\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5), metrics=['accuracy'])\n",
    "model.fit(tf_train, validation_data=tf_val, callbacks=callbacks_list, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/raw/test_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/raw/test_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./output/raw/test_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
